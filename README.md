<div align="center">

# 🚀 UIPro: Unleashing Superior Interaction Capability For GUI Agents

### 🎯 ICCV 2025 • Next-Generation AI GUI Automation

<p align="center">
  <img src="https://img.shields.io/badge/🔬_Research-ICCV_2025-FF6B6B?style=for-the-badge&labelColor=1A1A2E&color=FF6B6B" alt="Research Badge"/>
  <img src="https://img.shields.io/badge/🤖_AI-Multi--Modal-00D4FF?style=for-the-badge&labelColor=1A1A2E&color=00D4FF" alt="AI Badge"/>
  <img src="https://img.shields.io/badge/⚡_Performance-SOTA-FFD93D?style=for-the-badge&labelColor=1A1A2E&color=FFD93D" alt="Performance Badge"/>
</p>

<p align="center">
  <a href="https://arxiv.org/abs/2406.08487">
    <img src="https://img.shields.io/badge/📄_Paper-arXiv-B892FF?style=for-the-badge&logo=arxiv&logoColor=white&labelColor=1A1A2E" alt="Paper"/>
  </a>
  <a href="https://huggingface.co/collections/yifanzhang114/slime-665bcb2d0d71762b86fdbd2d">
    <img src="https://img.shields.io/badge/🤗_Models-Hugging_Face-FFB86C?style=for-the-badge&logo=huggingface&logoColor=white&labelColor=1A1A2E" alt="Models"/>
  </a>
  <a href="https://huggingface.co/datasets/yifanzhang114/SMR">
    <img src="https://img.shields.io/badge/📊_Dataset-Hugging_Face-6BCF7F?style=for-the-badge&logo=huggingface&logoColor=white&labelColor=1A1A2E" alt="Dataset"/>
  </a>
</p>

<img src="assets/uipro_github_banner.png" alt="UIPro Project Banner" style="border-radius: 15px; box-shadow: 0 10px 30px rgba(0,0,0,0.3); margin: 20px 0;"/>

</div>

---

<div align="center">

## 🌟 **Revolutionary GUI Agent Technology**

*UIPro represents a paradigm shift in GUI automation, achieving human-level interaction capabilities across multiple platforms through advanced AI*

</div>

### 🎨 **What Makes UIPro Special**

<table>
<tr>
<td width="50%" valign="top">

#### 🧠 **Intelligent Understanding**
- 📊 **20.6M** GUI understanding tasks
- 🖼️ **2.5M** unique screenshots  
- 🎯 **13** diverse task types
- 🔄 Advanced denoising pipeline

</td>
<td width="50%" valign="top">

#### ⚡ **Superior Performance**
- 🏆 **State-of-the-art** on multiple benchmarks
- 🚀 **68-85%** success rates across platforms
- 🎮 **Unified action space** framework
- 🌐 **Cross-platform** compatibility

</td>
</tr>
</table>

---

<div align="center">

## 🏗️ **Architecture & Training Pipeline**

*A two-stage revolutionary approach to GUI agent development*

</div>

<img src="assets/uipro_mainfigure.png" alt="UIPro Methodology Diagram" style="width: 100%; border-radius: 10px; margin: 20px 0;"/>

<div align="center">

### 🔄 **Two-Stage Training Process**

</div>

<table>
<tr>
<td width="50%" align="center">

#### 🎯 **Stage 1: GUI Understanding**
```
📚 Pre-training Phase
├── 🧠 Element grounding
├── 🔍 Function recognition  
├── 🎯 Intent mapping
└── 📊 Massive dataset learning
```

</td>
<td width="50%" align="center">

#### ⚡ **Stage 2: Agent Fine-tuning**
```
🎮 Action Execution Phase
├── 📋 Task planning
├── 🖱️ Precise interactions
├── 🌐 Cross-platform ops
└── 🎯 Unified action space
```

</td>
</tr>
</table>

---

<div align="center">

## 🎯 **Core Capabilities**

</div>

<details>
<summary><b>🧠 GUI Understanding Capabilities</b></summary>

<br>

| Capability | Description | Performance |
|------------|-------------|-------------|
| **🎯 Element Grounding** | Accurately locates UI elements based on descriptions | ⭐⭐⭐⭐⭐ |
| **🔍 Functionality Recognition** | Understands purpose and function of interface components | ⭐⭐⭐⭐⭐ |
| **🧭 Intent Mapping** | Connects user intentions to appropriate UI interactions | ⭐⭐⭐⭐⭐ |

</details>

<details>
<summary><b>🤖 GUI Agent Task Execution</b></summary>

<br>

| Capability | Description | Performance |
|------------|-------------|-------------|
| **📋 Task Planning** | Breaks down complex requests into actionable steps | ⭐⭐⭐⭐⭐ |
| **⚡ Action Execution** | Performs clicks, typing, scrolling with high precision | ⭐⭐⭐⭐⭐ |
| **🌐 Cross-Platform Navigation** | Seamless operation across different device types | ⭐⭐⭐⭐⭐ |

</details>

---

<div align="center">

## 📊 **Performance Benchmarks**

*Industry-leading results across all major GUI benchmarks*

</div>

### 🏆 **GUI Agent Task Evaluation**

<div align="center">

| 🎯 **Benchmark** | 🤖 **UIPro-SLiME (3B)** | 🚀 **UIPro-Qwen2VL (7B)** | 📊 **Metric** |
|:---------------:|:----------------------:|:------------------------:|:------------:|
| **AITW** | <span style="color: #00D4FF; font-weight: bold;">68.0%</span> | <span style="color: #FF6B6B; font-weight: bold;">70.4%</span> | Step SR |
| **AndroidControl** | <span style="color: #00D4FF; font-weight: bold;">61.1%</span> | <span style="color: #FF6B6B; font-weight: bold;">85.5%</span> | Step SR |
| **GUIAct-Web** | <span style="color: #00D4FF; font-weight: bold;">68.2%</span> | <span style="color: #FF6B6B; font-weight: bold;">69.1%</span> | Step SR |
| **Mind2Web** | <span style="color: #00D4FF; font-weight: bold;">28.7%</span> | <span style="color: #FF6B6B; font-weight: bold;">48.4%</span> | Step SR |

<small><i>Step Success Rate (Step SR) - Higher is better</i></small>

</div>

---

<div align="center">

## 🚀 **Quick Start Guide**

*Get up and running with UIPro in minutes*

</div>

### 📦 **Installation**

<details>
<summary><b>🔧 Setup Instructions</b></summary>

<br>

#### 1️⃣ **Clone Repository**
```bash
git clone https://github.com/ZJULiHongxin/UIPro.git
cd UIPro
```

#### 2️⃣ **Install Dependencies**
```bash
pip install -r requirements.txt
```

</details>

### 💡 **Basic Usage**

<details>
<summary><b>🎮 Quick Example</b></summary>

<br>

```python
from uipro import UIPro

# Initialize the model
model = UIPro.from_pretrained("uipro-qwen2vl-7b")

# Your GUI automation code here
```

</details>

---

<div align="center">

## 📚 **Dataset: The Foundation of Excellence**

*The world's largest and most comprehensive GUI understanding collection*

</div>

<div align="center">

### 📊 **Dataset Statistics**

| Metric | Value | Description |
|:------:|:-----:|:-----------:|
| 📝 **Task Samples** | **20.6M** | GUI understanding tasks |
| 🖼️ **Screenshots** | **2.5M** | Unique GUI screenshots |
| 🎯 **Elements** | **3.3M** | Clean GUI elements |
| 🔢 **Task Types** | **13** | Different task categories |

</div>

### 🏗️ **Data Compilation Pipeline**

<details>
<summary><b>🔧 General Setup Instructions</b></summary>

<br>

First of all, create a root folder saving all raw data and a folder saving processed training samples.

> **Note**: We also implemented a systematic denoising procedure to ensure data quality, removing up to 29% of noise from some data sources.

</details>

<details>
<summary><b>📱 Android in the Wild (AiTW)</b></summary>

<br>

First download the AiTW screenshots from [this URL](https://box.nju.edu.cn/f/96ba5115bae24eaaa44e/) and the annotations from [this URL](https://box.nju.edu.cn/f/1245c74fc09b4565a235/), and then unzip and place them in a folder like:

<details>
<summary><b>📂 Directory Structure</b></summary>

```
root/
├── AITW/
│   ├── aitw_data_test.json
│   ├── aitw_data_train.json
│   ├── aitw_data_val.json
│   └── aitw_images/
│       ├── general/
│       ├── gogleapps/
│       ├── install/
│       ├── single/
│       └── webshopping/
```

</details>

<details>
<summary><b>⚙️ Processing Instructions</b></summary>

Next, modify the `ROOT`, `SAVE_DIR`, `SPLIT`, `POINT_FORMAT` in `utils/data_utils/make_aitw_data/make_aitw_data.py` and then run:

```bash
python utils/data_utils/make_aitw_data/make_aitw_data.py
```

Finally, the processed training samples will be saved in `SAVE_DIR/AITW_processed`.

</details>

</details>

<details>
<summary><b>🦓 Android in the Zoo (AitZ)</b></summary>

<br>

First download the raw data according to the instructions in [the AitZ Github Repo](https://github.com/IMNearth/CoAT), and then unzip and place them in a folder like:

<details>
<summary><b>📂 Directory Structure</b></summary>

```
root/
├── AITZ/
│   ├── train/
│   │   ├── general/
│   │   ├── googleapps/
│   │   ├── install/
│   │   ├── single/
│   │   └── webshopping/
│   └── test/
│       ├── general/
│       ├── googleapps/
│       ├── install/
│       └── webshopping/
```

</details>

<details>
<summary><b>⚙️ Processing Instructions</b></summary>

Next, modify the `ROOT`, `SAVE_DIR`, `SPLIT`, `POINT_FORMAT` in `utils/data_utils/make_aitw_data/make_aitw_data.py` and then run:

```bash
python utils/data_utils/make_aitw_data/make_aitw_data.py
```

Finally, the processed training samples will be saved in `SAVE_DIR/AITW_processed`.

</details>

</details>

<details>
<summary><b>🎮 AndroidControl</b></summary>

<br>

First download the raw data according to the instructions in [the AndroidControl Github Repo](https://github.com/google-research/google-research/blob/master/android_control/README.md), and then unzip and place them in a folder like:

<details>
<summary><b>📂 Directory Structure</b></summary>

```
root/
├── AndroidControl/
│   ├── raw/
│   │   ├── android_control-00000-of-00020
│   │   ├── android_control-00001-of-00020
│   │   ├── ...
│   │   ├── android_control-00019-of-00020
│   │   └── splits.json
```

</details>

<details>
<summary><b>⚙️ Processing Instructions</b></summary>

Next, modify the `ANDROIDCONTROL_ROOT`, `SAVE_DIR`, `SPLIT`, `POINT_FORMAT` in `utils/data_utils/make_androidcontrol_data/make_androidcontrol_data.py` and then run:

```bash
python utils/data_utils/make_androidcontrol_data/make_androidcontrol_data.py
```

Finally, the processed training samples will be saved in `SAVE_DIR/AndroidControl_processed` and the extracted screenshot images will be saved in `ANDROIDCONTROL_ROOT/images`.

</details>

</details>

<details>
<summary><b>🌊 GUIOdyssey</b></summary>

<br>

First download the raw data according to the instructions in [the GUIOdyssey HF Repo](https://huggingface.co/datasets/hflqf88888/GUIOdyssey), and then unzip and place them in a folder like:

<details>
<summary><b>📂 Initial Directory Structure</b></summary>

```
root/
├── GUIOdyssey_raw/
│   ├── screenshots/
│   │   ├── data_0/
│   │   ├── data_1/
│   │   ├── data_2/
│   │   └── ...
│   ├── splits/
│   └── annotations/
```

</details>

<details>
<summary><b>📁 Reorganize Screenshots</b></summary>

Next, move all images in the `data_*` subfolders under `screenshots` to `screenshots`, like this:

```
root/
├── GUIOdyssey_raw/
│   ├── screenshots/
│   │   ├── 2386365564178401_9.png
│   │   ├── 5022534067657028_12.png
│   │   ├── 7287738713744873_13.png
│   │   └── ...
│   ├── splits/
│   └── annotations/
```

</details>

<details>
<summary><b>⚙️ Processing Instructions</b></summary>

Next, modify the `ANDROIDCONTROL_ROOT`, `SAVE_DIR`, `SPLIT`, `POINT_FORMAT` in `utils/data_utils/make_guiodyssey_data/make_guiodyssey_data.py` and then run:

```bash
python utils/data_utils/make_guiodyssey_data/make_guiodyssey_data.py
```

Finally, the processed training samples will be saved in `SAVE_DIR/GUIOdyssey_processed` and the extracted screenshot images will be saved in the appropriate directory.

</details>

</details>


<details>
<summary><b>AMEX</b></summary>

<br>

First download the raw data according to the instructions in [the AMEX HF Repo](https://huggingface.co/datasets/Yuxiang007/AMEX), and then unzip and place them in a folder like:

<details>
<summary><b>📂 Initial Directory Structure</b></summary>

```
root/
├── AMEX/
│   ├── element_anno/
│   ├── screenshot/
│   └── metadata/
```

</details>

<details>
<summary><b>⚙️ Processing Instructions</b></summary>

Next, modify the `ANDROIDCONTROL_ROOT`, `SAVE_DIR`, `SPLIT`, `POINT_FORMAT` in `utils/data_utils/make_guiodyssey_data/make_guiodyssey_data.py` and then run:

```bash
python utils/data_utils/make_guiodyssey_data/make_guiodyssey_data.py
```

Finally, the processed training samples will be saved in `SAVE_DIR/GUIOdyssey_processed` and the extracted screenshot images will be saved in the appropriate directory.

</details>

</details>


<details>
<summary><b>GUIAct</b></summary>

<br>

First download the raw data from [the GUIAct HF Repo](https://huggingface.co/datasets/yiye2023/GUIAct), and then place them in a folder like:

<details>
<summary><b>📂 Initial Directory Structure</b></summary>

```
root/
├── GUICourse/
│   ├── GUIAct/
│   │   ├── smartphone_test_data.json
│   │   ├── smartphone_test_images.parquet
│   │   ├── smartphone_train_data.json
│   │   └── ...
```

</details>

<details>
<summary><b>⚙️ Processing Instructions</b></summary>

Next, set `SUBTASK=['GUIAct']`, modify the `DATA_ROOT`, `SAVE_DIR`, `CURRENT_SPLIT`, `CURRENT_DEVICE_TYPE` in `utils/data_utils/make_guicourse_data/make_guicourse_data.py` and then run:

```bash
python utils/data_utils/make_guicourse_data/make_guicourse_data.py
```

Finally, the processed training samples will be saved in `SAVE_DIR/GUICourse_processed` and the extracted screenshot images will be saved in the appropriate directory.

</details>

</details>

<details>
<summary><b>GUIAct</b></summary>

<br>

First download the raw data from [the GUIAct HF Repo](https://huggingface.co/datasets/yiye2023/GUIAct), and then place them in a folder like:

<details>
<summary><b>📂 Initial Directory Structure</b></summary>

```
root/
├── GUICourse/
│   ├── GUIAct/
│   │   ├── smartphone_test_data.json
│   │   ├── smartphone_test_images.parquet
│   │   ├── smartphone_train_data.json
│   │   └── ...
```

</details>

<details>
<summary><b>⚙️ Processing Instructions</b></summary>

Next, modify the `DATA_ROOT`, `SAVE_DIR`, `SPLIT`, `DEVICE_TYPE` in `utils/data_utils/make_guicourse_data/make_guicourse_data.py` and then run:

```bash
python utils/data_utils/make_guicourse_data/make_guicourse_data.py
```

Finally, the processed training samples will be saved in `SAVE_DIR/GUICourse_processed` and the extracted screenshot images will be saved in the appropriate directory.

</details>

</details>

---

<div align="center">

## 🔬 **Technical Deep Dive**

*Advanced technical details for researchers and developers*

</div>

### 🎮 **Unified Action Space Design**

<details>
<summary><b>📱 Mobile Action Framework</b></summary>

<br>

```json
{
  "mobile_actions": [
    "tap", "long_press", "drag", "input_text",
    "navigate_home", "navigate_back", "navigate_recent",
    "press_enter", "swipe", "wait", "status_complete"
  ]
}
```

</details>

<details>
<summary><b>⚡ Unified Swipe Action</b></summary>

<br>

```json
{
  "action": "swipe",
  "start": [x, y],          // Starting coordinates
  "direction": "up",        // Movement direction  
  "distance": 200           // Swipe distance in pixels
}
```

</details>

---

<div align="center">

## 📝 **Citation**

*If you use UIPro in your research, please cite our paper*

</div>

```bibtex
@inproceedings{li2025uipro,
  title={UIPro: Unleashing Superior Interaction Capability For GUI Agents},
  author={Li, Hongxin and Su, Jingran and Chen, Jingfan and Ju, Zheng and Chen, Yuntao and Li, Qing and Zhang, Zhaoxiang},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  year={2025}
}
```

---

<div align="center">

## 👥 **Team & Acknowledgments**

*Special thanks to our research team and the open-source community*

<p>
This work was supported in part by the <b>National Key R&D Program of China</b> and the <b>National Natural Science Foundation of China</b>. We extend our gratitude to the open-source community for providing foundational datasets and tools that made this research possible.
</p>

<br>

---

<br>

## ⭐ **Star this repository if you find UIPro helpful!** ⭐

<a href="https://github.com/ZJULiHongxin/UIPro/stargazers">
  <img src="https://img.shields.io/github/stars/ZJULiHongxin/UIPro?style=for-the-badge&logo=github&logoColor=white&labelColor=1A1A2E&color=FFD93D" alt="GitHub Stars"/>
</a>

<br><br>

<p><i>🚀 Revolutionizing GUI automation, one interaction at a time</i></p>

</div>