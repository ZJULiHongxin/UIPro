<div align="center">

# ğŸš€ UIPro: Unleashing Superior Interaction Capability For GUI Agents

### ğŸ¯ ICCV 2025 â€¢ Next-Generation AI GUI Automation

<p align="center">
  <img src="https://img.shields.io/badge/ğŸ”¬_Research-ICCV_2025-FF6B6B?style=for-the-badge&labelColor=1A1A2E&color=FF6B6B" alt="Research Badge"/>
  <img src="https://img.shields.io/badge/ğŸ¤–_AI-Multi--Modal-00D4FF?style=for-the-badge&labelColor=1A1A2E&color=00D4FF" alt="AI Badge"/>
  <img src="https://img.shields.io/badge/âš¡_Performance-SOTA-FFD93D?style=for-the-badge&labelColor=1A1A2E&color=FFD93D" alt="Performance Badge"/>
</p>

<p align="center">
  <a href="https://arxiv.org/abs/2509.17328">
    <img src="https://img.shields.io/badge/ğŸ“„_Paper-arXiv-B892FF?style=for-the-badge&logo=arxiv&logoColor=white&labelColor=1A1A2E" alt="Paper"/>
  </a>
  <!-- <a href="https://huggingface.co/collections/yifanzhang114/slime-665bcb2d0d71762b86fdbd2d">
    <img src="https://img.shields.io/badge/ğŸ¤—_Models-Hugging_Face-FFB86C?style=for-the-badge&logo=huggingface&logoColor=white&labelColor=1A1A2E" alt="Models"/>
  </a> -->
  <!-- <a href="https://huggingface.co/datasets/yifanzhang114/SMR">
    <img src="https://img.shields.io/badge/ğŸ“Š_Dataset-Hugging_Face-6BCF7F?style=for-the-badge&logo=huggingface&logoColor=white&labelColor=1A1A2E" alt="Dataset"/>
  </a> -->
</p>

<img src="assets/uipro_github_banner.png" alt="UIPro Project Banner" style="border-radius: 15px; box-shadow: 0 10px 30px rgba(0,0,0,0.3); margin: 20px 0;"/>

</div>

## ğŸ“¢ News

- **[2025-06-28]** ğŸ‰ UIPro has been accepted to **ICCV 2025**!
- **[2025-11-23]** Uploaded [UIPro models](https://huggingface.co/HongxinLi/UIPro_1stage).
- **[2025-11-23]** Uploaded data processing scripts, and systematic denoising procedures for AITW, AITZ, MobileViews, WAE, WebUI, MultiUI, AndroidControl, GUIOdyssey, AMEX, GUIAct
- **[2025-11-27]** Uploaded data processing scripts, and systematic denoising procedures for SeeClick-Web, RefExp
- **[TODO]** Upload whole datasets

---

<div align="center">

## ğŸŒŸ **Revolutionary GUI Agent Technology**

*UIPro represents a paradigm shift in GUI automation, achieving human-level interaction capabilities across multiple platforms through advanced AI.*

</div>

### ğŸ¨ **What Makes UIPro Special**

<div align="center">

## ğŸ—ï¸ **Architecture & Training Pipeline**

*A two-stage revolutionary approach to GUI agent development*

<img src="assets/uipro_mainfigure.png" alt="UIPro Methodology Diagram" style="width: 100%; border-radius: 10px; margin: 20px 0;"/>

### ğŸ”„ **Two-Stage Training Process**

</div>

The training process involves a sophisticated pipeline designed to enhance both understanding and execution capabilities.

---

<div align="center">

## ğŸ¯ **Core Capabilities**

</div>

### ğŸ§  GUI Understanding Capabilities

| Capability | Description | Performance |
| :--- | :--- | :--- |
| **ğŸ¯ Element Grounding** | Accurately locates UI elements based on descriptions | â­â­â­â­â­ |
| **ğŸ” Functionality Recognition** | Understands purpose and function of interface components | â­â­â­â­â­ |
| **ğŸ§­ Intent Mapping** | Connects user intentions to appropriate UI interactions | â­â­â­â­â­ |

### ğŸ¤– GUI Agent Task Execution

| Capability | Description | Performance |
| :--- | :--- | :--- |
| **ğŸ“‹ Task Planning** | Breaks down complex requests into actionable steps | â­â­â­â­â­ |
| **âš¡ Action Execution** | Performs clicks, typing, scrolling with high precision | â­â­â­â­â­ |
| **ğŸŒ Cross-Platform Navigation** | Seamless operation across different device types | â­â­â­â­â­ |

---

<div align="center">

## ğŸ“Š **Performance Benchmarks**

*Industry-leading results across all major GUI benchmarks*

### ğŸ† **GUI Agent Task Evaluation**

| ğŸ¯ Benchmark | ğŸ¤– UIPro-SLiME (3B) | ğŸš€ UIPro-Qwen2VL (7B) | ğŸ“Š Metric |
| :---: | :---: | :---: | :---: |
| **AITW** | <span style="color: #00D4FF; font-weight: bold;">68.0%</span> | <span style="color: #FF6B6B; font-weight: bold;">70.4%</span> | Step SR |
| **AndroidControl** | <span style="color: #00D4FF; font-weight: bold;">61.1%</span> | <span style="color: #FF6B6B; font-weight: bold;">85.5%</span> | Step SR |
| **GUIAct-Web** | <span style="color: #00D4FF; font-weight: bold;">68.2%</span> | <span style="color: #FF6B6B; font-weight: bold;">69.1%</span> | Step SR |
| **Mind2Web** | <span style="color: #00D4FF; font-weight: bold;">28.7%</span> | <span style="color: #FF6B6B; font-weight: bold;">48.4%</span> | Step SR |

<small><i>Step Success Rate (Step SR) - Higher is better</i></small>

</div>

---

<div align="center">

## ğŸš€ **Quick Start Guide**

*Get up and running with UIPro in minutes*

</div>

### ğŸ“¦ **Installation**

<details>
<summary><b>ğŸ”§ Setup Instructions</b></summary>
<br>

#### 1ï¸âƒ£ **Clone Repository**

```bash
git clone https://github.com/ZJULiHongxin/UIPro.git
cd UIPro
```

#### 2ï¸âƒ£ **Install Dependencies**

```bash
pip install -r requirements.txt
```

</details>

<!-- ### ğŸ’¡ **Basic Usage**

<details>
<summary><b>ğŸ® Quick Example</b></summary>
<br>

```python
from uipro import UIPro

# Initialize the model
model = UIPro.from_pretrained("uipro-qwen2vl-7b")

# Your GUI automation code here
```

</details> -->

---

<div align="center">

## ğŸ“š **Dataset: The Foundation of Excellence**

*The world's largest and most comprehensive GUI understanding collection*

</div>

<div align="center">

### ğŸ“Š **Dataset Statistics**

| Metric | Value | Description |
| :---: | :---: | :---: |
| ğŸ“**Task Samples** | **20.6M** | GUI understanding tasks |
| ğŸ–¼ï¸**Screenshots** | **2.5M** | Unique GUI screenshots |
| ğŸ¯**Elements** | **3.3M** | Clean GUI elements |
| ğŸ”¢**Task Types** | **13** | Different task categories |

</div>

### ğŸ—ï¸ **Data Compilation Pipeline**

We provide comprehensive scripts to process various GUI datasets. Please follow the instructions below for each dataset.

> **Note**: We also implemented a systematic denoising procedure to ensure data quality, removing up to 29% of noise from some data sources.

<details>
<summary><b>ğŸ“± MobileViews</b></summary>
<br>

1. Download the MobileViews raw data from [HuggingFace](https://huggingface.co/datasets/mllmTeam/MobileViews) via:
   ```bash
   hf download mllmTeam/MobileViews --repo-type dataset --local-dir ./MobileViews
   ```
2. Unzip and organize the data as follows:
   ```
   root/
   â”œâ”€â”€ MobileViews/
   â”‚   â”œâ”€â”€ MobileViews_0-150000/
   â”‚   â”œâ”€â”€ MobileViews_0-150000.csv
   â”‚   â”œâ”€â”€ MobileViews_150001-291197/
   â”‚   â”œâ”€â”€ MobileViews_150001-291197.csv
   â”‚   â””â”€â”€ ...
   ```
   
3. Modify `MOBILEVIEWS_DIR`, `ROOT`, `SCALE` (coordinate scale), and `PROB_BOX` (proportion of the box-prediction samples) in `utils/data_utils/make_mobileviews_data/extract_and_generate_mobilebiews_data.py`.
4. Run the processing script (this may take ~48 hours due to the large number of screenshots):
   ```bash
   python utils/data_utils/make_mobileviews_data/extract_and_generate_mobilebiews_data.py
   ```
   Processed training samples will be saved in `ROOT/mobileviews_processed`.
5. Finally, run `utils/data_utils/make_mobileviews_data/run_generate_symlinks.sh` to create a unified folder for screenshots.

</details>

<details>
<summary><b>ğŸ“± WAE</b></summary>
<br>

1. Download the WAE raw data from [WAE DropBox](https://www.dropbox.com/scl/fo/im6hs48z43h86i80xr517/APE_cMh8qVX_l1Jf_kqArRA/ui30k?dl=0&rlkey=etueluh3jw5adnnntcgrqnqqq&subfolder_nav_tracking=1).

2. Merge, unzip and organize the data as follows:
   ```
   root/
   â”œâ”€â”€ WAE/
   â”‚   â”œâ”€â”€ COM.HSBFREE_25-output
   â”‚   â”œâ”€â”€ Com.sktelecom.minit_52-output
   â”‚   â”œâ”€â”€ Draziw.Button.Mines_71-output
   â”‚   â”œâ”€â”€ HBVerbrauchszaehler.lite_119-output
   â”‚   â””â”€â”€ ...
   ```

3. Modify `WAE_DIR`, `ROOT_DIR`, `SCALE` (coordinate scale), and `PROB_BOX` (proportion of the box-prediction samples) in `utils/data_utils/make_WAE_data/make_WAE_data.py`.

4. Run the processing script (this may take ~24 hours due to the large number of screenshots):
   ```
   python utils/data_utils/make_WAE_data/make_WAE_data.py
   ```
   Processed training samples will be saved in `ROOT/WAE_processed`.

</details>

<details>
<summary><b>ğŸ“± WebUI</b></summary>
<br>

1. Download the WebUI raw data from [biglab/webui-all](https://huggingface.co/datasets/biglab/webui-all).

2. Merge, unzip and organize the data as follows:
   ```
   root/
   â”œâ”€â”€ WebUI/
   â”‚   â”œâ”€â”€ dh2 (GUI metadata resulting from unzipping)
   â”‚   â”œâ”€â”€ WebUI_screenshots (A folder used to save processed GUI screenshots)
   ```

3. Modify `WEBUI_DIR`, `WEBUI_PROCESSED_IMG_DIR`, `ROOT`, `SCALE` (coordinate scale), and `PROB_BOX` (proportion of the box-prediction samples) in `utils/data_utils/make_webui_data/make_webui_data.py`.

4. Run the processing script (this may take ~24 hours due to the large number of screenshots):
   ```
   python utils/data_utils/make_webui_data/make_webui_data.py
   ```
   Processed training samples will be saved in `ROOT/WebUI_processed`.

</details>

<details>
<summary><b>ğŸ“± MultiUI</b></summary>
<br>

1. Download the MultiUI raw data from [neulab/MultiUI](https://huggingface.co/datasets/neulab/MultiUI).

2. Merge, unzip and organize the data as follows:
   ```
   root/
   â”œâ”€â”€ MultiUI/
   â”‚   â”œâ”€â”€ v0.6_5M
   â”‚   â”œâ”€â”€ v0.7_exclude_v0.6
   |   â”œâ”€â”€ v0.8_exclude_v0.7
   â”‚   â”œâ”€â”€ stage1_data.json
   â”‚   â”œâ”€â”€ stage1_data_10k.json
   |   â”œâ”€â”€ stage2_data_to_be_combined_with_general_data.json
   ```

3. Modify `MULTIUI_SAMPLE_FILE`, `IMG_DIR`, `SAVE_ROOT`, and `SCALE` (coordinate scale) in `utils/data_utils/make_multiui_data/make_multiui_data.py`.

4. Run the processing script (this may take ~2 hours due to the large number of screenshots):
   ```
   python utils/data_utils/make_multiui_data/make_multiui_data.py
   ```
   Processed training samples will be saved in `ROOT/MultiUI_processed`.

</details>

<details>
<summary><b>ğŸ“± SeeClick-Web</b></summary>
<br>

1. Download the SeeClick-Web raw data from [SeeClick-Web Annotation File](https://box.nju.edu.cn/f/3b0f6ccb8bed476c8e39/) and [SeeClick-Web Images](https://box.nju.edu.cn/f/6a804cf190dd490a808f/).

2. Unzip and organize the data as follows:
   ```
   root/
   â”œâ”€â”€ SeeClick-Web/
   â”‚   â”œâ”€â”€ 0a5c8a5b7d73de574f2a21f27dbc9a53.png
   â”‚   â”œâ”€â”€ 0a6dcd3f9e1907af232e2c038a866f74.png
   | ...
   ```

3. Modify `IMG_DIR`, `ANNO_FILE`, `SAVE_ROOT`, and `SCALE` (coordinate scale) in `utils/data_utils/make_seeclickweb_data/make_seeclickweb_data.py`.

4. Run the processing script (this may take ~24 hours due to the large number of screenshots):
   ```
   python utils/data_utils/make_seeclickweb_data/make_seeclickweb_data.py
   ```
   Processed training samples will be saved in `ROOT/SeeClick-Web_processed`.

</details>

<details>
<summary><b>ğŸ¤– Android in the Wild (AiTW)</b></summary>
<br>

1. Download AiTW screenshots from [here](https://box.nju.edu.cn/f/96ba5115bae24eaaa44e/) and annotations from [here](https://box.nju.edu.cn/f/1245c74fc09b4565a235/).
2. Organize the data:
   ```
   root/
   â”œâ”€â”€ AITW/
   â”‚   â”œâ”€â”€ aitw_data_test.json
   â”‚   â”œâ”€â”€ aitw_data_train.json
   â”‚   â”œâ”€â”€ aitw_data_val.json
   â”‚   â””â”€â”€ aitw_images/
   â”‚       â”œâ”€â”€ general/
   â”‚       â”œâ”€â”€ googleapps/
   â”‚       â”œâ”€â”€ install/
   â”‚       â”œâ”€â”€ single/
   â”‚       â””â”€â”€ webshopping/
   ```
3. Modify `ROOT`, `SAVE_DIR`, `SPLIT`, and `POINT_FORMAT` in `utils/data_utils/make_aitw_data/make_aitw_data.py`.
4. Run the script:
   ```bash
   python utils/data_utils/make_aitw_data/make_aitw_data.py
   ```
   Processed samples will be saved in `SAVE_DIR/AITW_processed`.

</details>

<details>
<summary><b>ğŸ¦“ Android in the Zoo (AitZ)</b></summary>
<br>

1. Download raw data following instructions in the [AitZ Github Repo](https://github.com/IMNearth/CoAT).
2. Organize the data:
   ```
   root/
   â”œâ”€â”€ AITZ/
   â”‚   â”œâ”€â”€ train/
   â”‚   â”‚   â”œâ”€â”€ general/
   â”‚   â”‚   â”œâ”€â”€ googleapps/
   â”‚   â”‚   â”œâ”€â”€ install/
   â”‚   â”‚   â”œâ”€â”€ single/
   â”‚   â”‚   â””â”€â”€ webshopping/
   â”‚   â””â”€â”€ test/
   â”‚       â”œâ”€â”€ general/
   â”‚       â”œâ”€â”€ googleapps/
   â”‚       â”œâ”€â”€ install/
   â”‚       â””â”€â”€ webshopping/
   ```
3. Modify `ROOT`, `SAVE_DIR`, `SCALE`, `SPLIT`, and `USE_ACTION_REFEXP` in `utils/data_utils/make_aitz_data/make_aitz_data.py`.
4. Run the script:
   ```bash
   python utils/data_utils/make_aitz_data/make_aitz_data.py
   ```
   Processed samples will be saved in `SAVE_DIR/AITZ_processed`.

</details>

<details>
<summary><b>ğŸ® AndroidControl</b></summary>
<br>

1. Download raw data following instructions in the [AndroidControl Github Repo](https://github.com/google-research/google-research/blob/master/android_control/README.md).
2. Organize the data:
   ```
   root/
   â”œâ”€â”€ AndroidControl/
   â”‚   â”œâ”€â”€ raw/
   â”‚   â”‚   â”œâ”€â”€ android_control-00000-of-00020
   â”‚   â”‚   â”œâ”€â”€ android_control-00001-of-00020
   â”‚   â”‚   â”œâ”€â”€ ...
   â”‚   â”‚   â”œâ”€â”€ android_control-00019-of-00020
   â”‚   â”‚   â””â”€â”€ splits.json
   ```
3. Modify `ANDROIDCONTROL_ROOT`, `SAVE_DIR`, `SPLIT`, and `POINT_FORMAT` in `utils/data_utils/make_androidcontrol_data/make_androidcontrol_data.py`.
4. Run the script:
   ```bash
   python utils/data_utils/make_androidcontrol_data/make_androidcontrol_data.py
   ```
   Processed samples will be saved in `SAVE_DIR/AndroidControl_processed`.

</details>

<details>
<summary><b>ğŸŒŠ GUIOdyssey</b></summary>
<br>

1. Download raw data from the [GUIOdyssey HF Repo](https://huggingface.co/datasets/hflqf88888/GUIOdyssey).
2. Organize the data:
   ```
   root/
   â”œâ”€â”€ GUIOdyssey_raw/
   â”‚   â”œâ”€â”€ screenshots/
   â”‚   â”‚   â”œâ”€â”€ 2386365564178401_9.png
   â”‚   â”‚   â”œâ”€â”€ 5022534067657028_12.png
   â”‚   â”‚   â”œâ”€â”€ 7287738713744873_13.png
   â”‚   â”‚   â””â”€â”€ ...
   â”‚   â”œâ”€â”€ splits/
   â”‚   â””â”€â”€ annotations/
   ```
3. Move all images from `data_*` subfolders in `screenshots` directly to `screenshots`.
4. Modify `DATA_ROOT`, `SAVE_ROOT`, and `SPLIT` in `utils/data_utils/make_guiodyssey_data/make_guiodyssey_data.py`.
5. Run the script:
   ```bash
   python utils/data_utils/make_guiodyssey_data/make_guiodyssey_data.py
   ```
   Processed samples will be saved in `SAVE_ROOT/GUIOdyssey_processed`.

</details>

<details>
<summary><b>ğŸ’³ AMEX</b></summary>
<br>

1. Download raw data from the [AMEX HF Repo](https://huggingface.co/datasets/Yuxiang007/AMEX).
2. Organize the data:
   ```
   root/
   â”œâ”€â”€ AMEX/
   â”‚   â”œâ”€â”€ element_anno/
   â”‚   â”œâ”€â”€ screenshot/
   â”‚   â””â”€â”€ metadata/
   ```
3. Modify `DATA_ROOT`, `SAVE_ROOT`, and `SPLIT` in `utils/data_utils/make_amex_data/make_amex_data.py`.
4. Run the script:
   ```bash
   python utils/data_utils/make_amex_data/make_amex_data.py
   ```
   Processed samples will be saved in `SAVE_ROOT/AMEX_processed`.

</details>

<details>
<summary><b>ğŸ­ GUIAct</b></summary>
<br>

1. Download raw data from the [GUIAct HF Repo](https://huggingface.co/datasets/yiye2023/GUIAct).
2. Organize the data:
   ```
   root/
   â”œâ”€â”€ GUICourse/
   â”‚   â”œâ”€â”€ GUIAct/
   â”‚   â”‚   â”œâ”€â”€ smartphone_test_data.json
   â”‚   â”‚   â”œâ”€â”€ smartphone_test_images.parquet
   â”‚   â”‚   â”œâ”€â”€ smartphone_train_data.json
   â”‚   â”‚   â””â”€â”€ ...
   ```
3. Modify `DATA_ROOT`, `SAVE_DIR`, `CURRENT_SPLIT`, and `CURRENT_DEVICE_TYPE` in the `DatasetConfig` class within `utils/data_utils/make_guicourse_data/make_guicourse_data.py`.
4. Run the script:
   ```bash
   python utils/data_utils/make_guicourse_data/make_guicourse_data.py
   ```
   Processed samples will be saved in `SAVE_DIR`.

</details>

---

<div align="center">

## ğŸ”¬ **Technical Deep Dive**

*Advanced technical details for researchers and developers*

</div>

### ğŸ® **Unified Action Space Design**

<details>
<summary><b>ğŸ“± Mobile Action Framework</b></summary>
<br>

```json
{
  "mobile_actions": [
    "tap", "long_press", "drag", "input_text",
    "navigate_home", "navigate_back", "navigate_recent",
    "press_enter", "swipe", "wait", "status_complete"
  ]
}
```

</details>

<details>
<summary><b>âš¡ Unified Swipe Action</b></summary>
<br>

```json
{
  "action": "swipe",
  "start": [x, y],          // Starting coordinates
  "direction": "up",        // Movement direction  
  "distance": 200           // Swipe distance in pixels
}
```

</details>

---

<div align="center">

## ğŸ“ **Citation**

*If you use UIPro in your research, please cite our paper*

</div>

```bibtex
@inproceedings{li2025uipro,
  title={UIPro: Unleashing Superior Interaction Capability For GUI Agents},
  author={Li, Hongxin and Su, Jingran and Chen, Jingfan and Ju, Zheng and Chen, Yuntao and Li, Qing and Zhang, Zhaoxiang},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  year={2025}
}
```

---

<div align="center">

## ğŸ‘¥ **Team & Acknowledgments**

*Special thanks to our research team and the open-source community*

<p>
This work was supported in part by the <b>National Key R&D Program of China</b> and the <b>National Natural Science Foundation of China</b>. We extend our gratitude to the open-source community for providing foundational datasets and tools that made this research possible.
</p>

<br>

---

<br>

## â­ **Star this repository if you find UIPro helpful!** â­

<a href="https://github.com/ZJULiHongxin/UIPro/stargazers">
  <img src="https://img.shields.io/github/stars/ZJULiHongxin/UIPro?style=for-the-badge&logo=github&logoColor=white&labelColor=1A1A2E&color=FFD93D" alt="GitHub Stars"/>
</a>

<br><br>

<p><i>ğŸš€ Revolutionizing GUI automation, one interaction at a time</i></p>

</div>
