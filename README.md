<div align="center">

# ğŸš€ UIPro: Unleashing Superior Interaction Capability For GUI Agents

### ğŸ¯ ICCV 2025 â€¢ Next-Generation AI GUI Automation

<p align="center">
  <img src="https://img.shields.io/badge/ğŸ”¬_Research-ICCV_2025-FF6B6B?style=for-the-badge&labelColor=1A1A2E&color=FF6B6B" alt="Research Badge"/>
  <img src="https://img.shields.io/badge/ğŸ¤–_AI-Multi--Modal-00D4FF?style=for-the-badge&labelColor=1A1A2E&color=00D4FF" alt="AI Badge"/>
  <!-- <img src="https://img.shields.io/badge/âš¡_Performance-SOTA-FFD93D?style=for-the-badge&labelColor=1A1A2E&color=FFD93D" alt="Performance Badge"/> -->
</p>

<p align="center">
  <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Li_UIPro_Unleashing_Superior_Interaction_Capability_For_GUI_Agents_ICCV_2025_paper.html">
    <img src="https://img.shields.io/badge/ğŸ“„_Paper-arXiv-B892FF?style=for-the-badge&logo=arxiv&logoColor=white&labelColor=1A1A2E" alt="Paper"/>
  </a>
  <!-- <a href="https://huggingface.co/collections/yifanzhang114/slime-665bcb2d0d71762b86fdbd2d">
    <img src="https://img.shields.io/badge/ğŸ¤—_Models-Hugging_Face-FFB86C?style=for-the-badge&logo=huggingface&logoColor=white&labelColor=1A1A2E" alt="Models"/>
  </a> -->
  <!-- <a href="https://huggingface.co/datasets/yifanzhang114/SMR">
    <img src="https://img.shields.io/badge/ğŸ“Š_Dataset-Hugging_Face-6BCF7F?style=for-the-badge&logo=huggingface&logoColor=white&labelColor=1A1A2E" alt="Dataset"/>
  </a> -->
</p>

<img src="assets/uipro_github_banner.png" alt="UIPro Project Banner" style="border-radius: 15px; box-shadow: 0 10px 30px rgba(0,0,0,0.3); margin: 20px 0;"/>

</div>

## ğŸ“¢ News

- **[2025-06-28]** ğŸ‰ UIPro has been accepted to **ICCV 2025**!
- **[2025-11-23]** Uploaded [UIPro Grounding models](https://huggingface.co/HongxinLi/UIPro_1stage).
- **[2025-11-23]** Uploaded data processing scripts, and systematic denoising procedures for AITW, AITZ, MobileViews, WAE, WebUI, MultiUI, AndroidControl, GUIOdyssey, AMEX, GUIAct
- **[2025-11-27]** Uploaded data processing scripts, and systematic denoising procedures for SeeClick-Web, RefExp
- **[2025-12-10]** Uploaded data processing scripts, and systematic denoising procedures for MOTIF, RefExp, and GUIEnv
- **[2025-12-15]** Uploaded [UIPro Web Agent model](https://huggingface.co/HongxinLi/UIPro-7B_Stage2_Web) and [UIPro Mobile Agent model](https://huggingface.co/HongxinLi/UIPro-7B_Stage2_Mobile).
- **[2026-02-03]** Uploaded evaluation scripts.

- **[TODO]** Upload whole datasets

---

<div align="center">

## ğŸŒŸ **Revolutionary GUI Agent Technology**

*UIPro represents a paradigm shift in GUI automation, achieving human-level interaction capabilities across multiple platforms through advanced AI.*

</div>

### ğŸ¨ **What Makes UIPro Special**

<div align="center">

## ğŸ—ï¸ **Architecture & Training Pipeline**

*A two-stage revolutionary approach to GUI agent development*

<img src="assets/uipro_mainfigure.png" alt="UIPro Methodology Diagram" style="width: 100%; border-radius: 10px; margin: 20px 0;"/>

### ğŸ”„ **Two-Stage Training Process**

</div>

The training process involves a sophisticated pipeline designed to enhance both understanding and execution capabilities.

---

<div align="center">

## ğŸ¯ **Core Capabilities**

</div>

### ğŸ§  GUI Understanding Capabilities

| Capability | Description | Performance |
| :--- | :--- | :--- |
| **ğŸ¯ Element Grounding** | Accurately locates UI elements based on descriptions | â­â­â­â­â­ |
| **ğŸ” Functionality Recognition** | Understands purpose and function of interface components | â­â­â­â­â­ |
| **ğŸ§­ Intent Mapping** | Connects user intentions to appropriate UI interactions | â­â­â­â­â­ |

### ğŸ¤– GUI Agent Task Execution

| Capability | Description | Performance |
| :--- | :--- | :--- |
| **ğŸ“‹ Task Planning** | Breaks down complex requests into actionable steps | â­â­â­â­â­ |
| **âš¡ Action Execution** | Performs clicks, typing, scrolling with high precision | â­â­â­â­â­ |
| **ğŸŒ Cross-Platform Navigation** | Seamless operation across different device types | â­â­â­â­â­ |

---

<div align="center">

## ğŸ“Š **Performance Benchmarks**

*Industry-leading results across all major GUI benchmarks*

### ğŸ† **GUI Agent Task Evaluation**

| ğŸ¯ Benchmark | ğŸ¤– UIPro-SLiME (3B) | ğŸš€ UIPro-Qwen2VL (7B) | ğŸ“Š Metric |
| :---: | :---: | :---: | :---: |
| **AITW** | <span style="color: #00D4FF; font-weight: bold;">68.0%</span> | <span style="color: #FF6B6B; font-weight: bold;">70.4%</span> | Step SR |
| **AndroidControl** | <span style="color: #00D4FF; font-weight: bold;">61.1%</span> | <span style="color: #FF6B6B; font-weight: bold;">85.5%</span> | Step SR |
| **GUIAct-Web** | <span style="color: #00D4FF; font-weight: bold;">68.2%</span> | <span style="color: #FF6B6B; font-weight: bold;">69.1%</span> | Step SR |
| **Mind2Web** | <span style="color: #00D4FF; font-weight: bold;">28.7%</span> | <span style="color: #FF6B6B; font-weight: bold;">48.4%</span> | Step SR |

<small><i>Step Success Rate (Step SR) - Higher is better</i></small>

</div>

---



## ğŸš€ **Quick Start Guide**

*Get up and running with UIPro in minutes*

</div>

### ğŸ“¦ **Installation**

<details>
<summary><b>ğŸ”§ Setup Instructions</b></summary>
<br>

#### 1ï¸âƒ£ **Clone Repository**

```bash
git clone https://github.com/ZJULiHongxin/UIPro.git
cd UIPro
```

#### 2ï¸âƒ£ **Install Dependencies**

```bash
pip install -r requirements.txt
```

</details>

<!-- ### ğŸ’¡ **Basic Usage**

<details>
<summary><b>ğŸ® Quick Example</b></summary>
<br>

```python
from uipro import UIPro

# Initialize the model
model = UIPro.from_pretrained("uipro-qwen2vl-7b")

# Your GUI automation code here
```

</details> -->

---

<div align="center">

## ğŸ“š **Dataset: The Foundation of Excellence**

*The world's largest and most comprehensive GUI understanding collection*

</div>

<div align="center">

### ğŸ“Š **Dataset Statistics**

| Metric | Value | Description |
| :---: | :---: | :---: |
| ğŸ“**Task Samples** | **20.6M** | GUI understanding tasks |
| ğŸ–¼ï¸**Screenshots** | **2.5M** | Unique GUI screenshots |
| ğŸ¯**Elements** | **3.3M** | Clean GUI elements |
| ğŸ”¢**Task Types** | **13** | Different task categories |

</div>

### ğŸ—ï¸ **Data Compilation Pipeline**

We provide comprehensive scripts to process various GUI datasets. Please follow the instructions below for each dataset.

> **Note**: We also implemented a systematic denoising procedure to ensure data quality, removing up to 29% of noise from some data sources.

#### GUI Understanding SFT Data Processing

<details>
<summary><b>ğŸ“± MobileViews</b></summary>
<br>

1. Download the MobileViews raw data from [HuggingFace](https://huggingface.co/datasets/mllmTeam/MobileViews) via:
   ```bash
   hf download mllmTeam/MobileViews --repo-type dataset --local-dir ./MobileViews
   ```
2. Unzip and organize the data as follows:
   ```
   root/
   â”œâ”€â”€ MobileViews/
   â”‚   â”œâ”€â”€ MobileViews_0-150000/
   â”‚   â”œâ”€â”€ MobileViews_0-150000.csv
   â”‚   â”œâ”€â”€ MobileViews_150001-291197/
   â”‚   â”œâ”€â”€ MobileViews_150001-291197.csv
   â”‚   â””â”€â”€ ...
   ```
   
3. Modify `MOBILEVIEWS_DIR`, `ROOT`, `SCALE` (coordinate scale), and `PROB_BOX` (proportion of the box-prediction samples) in `utils/data_utils/make_mobileviews_data/extract_and_generate_mobilebiews_data.py`.
4. Run the processing script (this may take ~48 hours due to the large number of screenshots):
   ```bash
   python utils/data_utils/make_mobileviews_data/extract_and_generate_mobilebiews_data.py
   ```
   Processed training samples will be saved in `ROOT/mobileviews_processed`.
5. Finally, run `utils/data_utils/make_mobileviews_data/run_generate_symlinks.sh` to create a unified folder for screenshots.

</details>

<details>
<summary><b>ğŸ“± WAE</b></summary>
<br>

1. Download the WAE raw data from [WAE DropBox](https://www.dropbox.com/scl/fo/im6hs48z43h86i80xr517/APE_cMh8qVX_l1Jf_kqArRA/ui30k?dl=0&rlkey=etueluh3jw5adnnntcgrqnqqq&subfolder_nav_tracking=1).

2. Merge, unzip and organize the data as follows:
   ```
   root/
   â”œâ”€â”€ WAE/
   â”‚   â”œâ”€â”€ COM.HSBFREE_25-output
   â”‚   â”œâ”€â”€ Com.sktelecom.minit_52-output
   â”‚   â”œâ”€â”€ Draziw.Button.Mines_71-output
   â”‚   â”œâ”€â”€ HBVerbrauchszaehler.lite_119-output
   â”‚   â””â”€â”€ ...
   ```

3. Modify `WAE_DIR`, `ROOT_DIR`, `SCALE` (coordinate scale), and `PROB_BOX` (proportion of the box-prediction samples) in `utils/data_utils/make_WAE_data/make_WAE_data.py`.

4. Run the processing script (this may take ~24 hours due to the large number of screenshots):
   ```
   python utils/data_utils/make_WAE_data/make_WAE_data.py
   ```
   Processed training samples will be saved in `ROOT/WAE_processed`.

</details>

<details>
<summary><b>ğŸ“± WebUI</b></summary>
<br>

1. Download the WebUI raw data from [biglab/webui-all](https://huggingface.co/datasets/biglab/webui-all).

2. Merge, unzip and organize the data as follows:
   ```
   root/
   â”œâ”€â”€ WebUI/
   â”‚   â”œâ”€â”€ dh2 (GUI metadata resulting from unzipping)
   â”‚   â”œâ”€â”€ WebUI_screenshots (A folder used to save processed GUI screenshots)
   ```

3. Modify `WEBUI_DIR`, `WEBUI_PROCESSED_IMG_DIR`, `ROOT`, `SCALE` (coordinate scale), and `PROB_BOX` (proportion of the box-prediction samples) in `utils/data_utils/make_webui_data/make_webui_data.py`.

4. Run the processing script (this may take ~24 hours due to the large number of screenshots):
   ```
   python utils/data_utils/make_webui_data/make_webui_data.py
   ```
   Processed training samples will be saved in `ROOT/WebUI_processed`.

</details>

<details>
<summary><b>ğŸ“± MultiUI</b></summary>
<br>

1. Download the MultiUI raw data from [neulab/MultiUI](https://huggingface.co/datasets/neulab/MultiUI).

2. Merge, unzip and organize the data as follows:
   ```
   root/
   â”œâ”€â”€ MultiUI/
   â”‚   â”œâ”€â”€ v0.6_5M
   â”‚   â”œâ”€â”€ v0.7_exclude_v0.6
   |   â”œâ”€â”€ v0.8_exclude_v0.7
   â”‚   â”œâ”€â”€ stage1_data.json
   â”‚   â”œâ”€â”€ stage1_data_10k.json
   |   â”œâ”€â”€ stage2_data_to_be_combined_with_general_data.json
   ```

3. Modify `MULTIUI_SAMPLE_FILE`, `IMG_DIR`, `SAVE_ROOT`, and `SCALE` (coordinate scale) in `utils/data_utils/make_multiui_data/make_multiui_data.py`.

4. Run the processing script (this may take ~2 hours due to the large number of screenshots):
   ```
   python utils/data_utils/make_multiui_data/make_multiui_data.py
   ```
   Processed training samples will be saved in `ROOT/MultiUI_processed`.

</details>

<details>
<summary><b>ğŸ“± SeeClick-Web</b></summary>
<br>

1. Download the SeeClick-Web raw data from [SeeClick-Web Annotation File](https://box.nju.edu.cn/f/3b0f6ccb8bed476c8e39/) and [SeeClick-Web Images](https://box.nju.edu.cn/f/6a804cf190dd490a808f/).

2. Unzip and organize the data as follows:
   ```
   root/
   â”œâ”€â”€ SeeClick-Web/
   â”‚   â”œâ”€â”€ 0a5c8a5b7d73de574f2a21f27dbc9a53.png
   â”‚   â”œâ”€â”€ 0a6dcd3f9e1907af232e2c038a866f74.png
   | ...
   ```

3. Modify `IMG_DIR`, `ANNO_FILE`, `SAVE_ROOT`, and `SCALE` (coordinate scale) in `utils/data_utils/make_seeclickweb_data/make_seeclickweb_data.py`.

4. Run the processing script (this may take ~24 hours due to the large number of screenshots):
   ```
   python utils/data_utils/make_seeclickweb_data/make_seeclickweb_data.py
   ```
   Processed training samples will be saved in `ROOT/SeeClick-Web_processed`.

</details>

<details>
<summary><b>ğŸ“± GUIEnv</b></summary>
<br>

1. Download the GUIEnv raw data from [yiye2023/GUIEnv](https://huggingface.co/datasets/yiye2023/GUIEnv).

2. Unzip and organize the data as follows:
   ```
   root/
   â”‚   â””â”€â”€ GUICourse/
   â”‚            â””â”€â”€ GUIEnv/
   â”‚                    â”œâ”€â”€ imgs
   â”‚                    â”œâ”€â”€ ocr_grounding_train_stage2_images.parquet
   â”‚                    â”œâ”€â”€ ocr_grounding_train_stage2_data.json
   â”‚                    â”œâ”€â”€ ocr_grounding_train_stage1_images.parquet
   â”‚                    â”œâ”€â”€ ocr_grounding_train_stage1_data.json
   â”‚                    â”œâ”€â”€ ocr_grounding_test_images.parquet
   â”‚                    â””â”€â”€ ocr_grounding_test_data.json

   ```

3. Modify `SUBTASK`, `CURRENT_SPLIT`, `DATA_ROOT`, `SAVE_DIR`, `ENABLE_TEXTLOC`, `ENABLE_OCR`, `ENABLE_INTENT_GND`, and `SCALE` (coordinate scale) in `utils/data_utils/make_refexp_data/make_refexp_data.py`.

4. Run the processing script:
   ```
   python utils/data_utils/make_refexp_data/make_refexp_data.py
   ```
   Processed training samples will be saved in `ROOT/RefExp_processed`.

</details>

<details>
<summary><b>ğŸ“± RefExp</b></summary>
<br>

1. Download the RICO image data from [SeeClick RICO Data](https://box.nju.edu.cn/f/7ae5e9bd4bf840d4add3/).

2. Unzip and organize the data as follows:
   ```
   root/
   â”‚   â””â”€â”€ rico/
   â”‚       â”œâ”€â”€ 72197.jpg
   â”‚       â”œâ”€â”€ ...
   â”‚       â”œâ”€â”€ 71949.json
   â”‚       â””â”€â”€ ...

   ```

3. Modify `IMG_DIR`, `SAVE_ROOT`, , and `SCALE` (coordinate scale) in `utils/data_utils/make_refexp_data/make_refexp_data.py`.

4. Run the processing script:
   ```
   python utils/data_utils/make_refexp_data/make_refexp_data.py
   ```
   Processed training samples will be saved in `ROOT/RefExp_processed`.

</details>

<details>
<summary><b>ğŸ“± MOTIF</b></summary>
<br>

1. Download the MOTIF image data from [HongxinLi/MOTIF](https://huggingface.co/datasets/HongxinLi/MOTIF).

2. Unzip and organize the data as follows:
   ```
   root/
   â”‚   â””â”€â”€ motif/

   ```

3. Modify `IMG_DIR`, `SAVE_ROOT_DIR`, and `SCALE` (coordinate scale) in `utils/data_utils/make_motif_data/make_motif_data.py`.

4. Run the processing script:
   ```
   python utils/data_utils/make_motif_data/make_motif_data.py
   ```
   Processed training samples will be saved in `ROOT/MOTIF_processed`.

</details>

<details>
<summary><b>ğŸ“± OmniAct</b></summary>
<br>

1. Download the OmniAct raw data from [Writer/omniact](https://huggingface.co/datasets/Writer/omniact).

2. Unzip and organize the data as follows:
   ```
   root/
   â”œâ”€â”€ OmniAct/
   â”‚   â””â”€â”€ data/
   â”‚       â”œâ”€â”€ tasks/
   â”‚       â”œâ”€â”€ metadata/
   â”‚       â””â”€â”€ data/
   ```

3. Modify `ROOT_DIR`, `SAVE_ROOT_DIR`, `SPLIT`, and `SCALE` (coordinate scale) in `utils/data_utils/make_omniact_data/make_omniact_data.py`.

4. Run the processing script:
   ```
   python utils/data_utils/make_omniact_data/make_omniact_data.py
   ```
   Processed training samples will be saved in `ROOT/OmniAct_processed`.

</details>

#### Agentic SFT Data Processing

<details>
<summary><b>ğŸ¤– Android in the Wild (AiTW)</b></summary>
<br>

1. Download AiTW screenshots from [here](https://box.nju.edu.cn/f/96ba5115bae24eaaa44e/) and annotations from [here](https://box.nju.edu.cn/f/1245c74fc09b4565a235/).
2. Organize the data:
   ```
   root/
   â”œâ”€â”€ AITW/
   â”‚   â”œâ”€â”€ aitw_data_test.json
   â”‚   â”œâ”€â”€ aitw_data_train.json
   â”‚   â”œâ”€â”€ aitw_data_val.json
   â”‚   â””â”€â”€ aitw_images/
   â”‚       â”œâ”€â”€ general/
   â”‚       â”œâ”€â”€ googleapps/
   â”‚       â”œâ”€â”€ install/
   â”‚       â”œâ”€â”€ single/
   â”‚       â””â”€â”€ webshopping/
   ```
3. Modify `ROOT`, `SAVE_DIR`, `SPLIT`, and `POINT_FORMAT` in `utils/data_utils/make_aitw_data/make_aitw_data.py`.
4. Run the script:
   ```bash
   python utils/data_utils/make_aitw_data/make_aitw_data.py
   ```
   Processed samples will be saved in `SAVE_DIR/AITW_processed`.

</details>

<details>
<summary><b>ğŸ¦“ Android in the Zoo (AitZ)</b></summary>
<br>

1. Download raw data following instructions in the [AitZ Github Repo](https://github.com/IMNearth/CoAT).
2. Organize the data:
   ```
   root/
   â”œâ”€â”€ AITZ/
   â”‚   â”œâ”€â”€ train/
   â”‚   â”‚   â”œâ”€â”€ general/
   â”‚   â”‚   â”œâ”€â”€ googleapps/
   â”‚   â”‚   â”œâ”€â”€ install/
   â”‚   â”‚   â”œâ”€â”€ single/
   â”‚   â”‚   â””â”€â”€ webshopping/
   â”‚   â””â”€â”€ test/
   â”‚       â”œâ”€â”€ general/
   â”‚       â”œâ”€â”€ googleapps/
   â”‚       â”œâ”€â”€ install/
   â”‚       â””â”€â”€ webshopping/
   ```
3. Modify `ROOT`, `SAVE_DIR`, `SCALE`, `SPLIT`, and `USE_ACTION_REFEXP` in `utils/data_utils/make_aitz_data/make_aitz_data.py`.
4. Run the script:
   ```bash
   python utils/data_utils/make_aitz_data/make_aitz_data.py
   ```
   Processed samples will be saved in `SAVE_DIR/AITZ_processed`.

</details>

<details>
<summary><b>ğŸ® AndroidControl</b></summary>
<br>

1. Download raw data following instructions in the [AndroidControl Github Repo](https://github.com/google-research/google-research/blob/master/android_control/README.md).
2. Organize the data:
   ```
   root/
   â”œâ”€â”€ AndroidControl/
   â”‚   â”œâ”€â”€ raw/
   â”‚   â”‚   â”œâ”€â”€ android_control-00000-of-00020
   â”‚   â”‚   â”œâ”€â”€ android_control-00001-of-00020
   â”‚   â”‚   â”œâ”€â”€ ...
   â”‚   â”‚   â”œâ”€â”€ android_control-00019-of-00020
   â”‚   â”‚   â””â”€â”€ splits.json
   ```
3. Modify `ANDROIDCONTROL_ROOT`, `SAVE_DIR`, `SPLIT`, and `POINT_FORMAT` in `utils/data_utils/make_androidcontrol_data/make_androidcontrol_data.py`.
4. Run the script:
   ```bash
   python utils/data_utils/make_androidcontrol_data/make_androidcontrol_data.py
   ```
   Processed samples will be saved in `SAVE_DIR/AndroidControl_processed`.

</details>

<details>
<summary><b>ğŸŒŠ GUIOdyssey</b></summary>
<br>

1. Download raw data from the [GUIOdyssey HF Repo](https://huggingface.co/datasets/hflqf88888/GUIOdyssey).
2. Organize the data:
   ```
   root/
   â”œâ”€â”€ GUIOdyssey_raw/
   â”‚   â”œâ”€â”€ screenshots/
   â”‚   â”‚   â”œâ”€â”€ 2386365564178401_9.png
   â”‚   â”‚   â”œâ”€â”€ 5022534067657028_12.png
   â”‚   â”‚   â”œâ”€â”€ 7287738713744873_13.png
   â”‚   â”‚   â””â”€â”€ ...
   â”‚   â”œâ”€â”€ splits/
   â”‚   â””â”€â”€ annotations/
   ```
3. Move all images from `data_*` subfolders in `screenshots` directly to `screenshots`.
4. Modify `DATA_ROOT`, `SAVE_ROOT`, and `SPLIT` in `utils/data_utils/make_guiodyssey_data/make_guiodyssey_data.py`.
5. Run the script:
   ```bash
   python utils/data_utils/make_guiodyssey_data/make_guiodyssey_data.py
   ```
   Processed samples will be saved in `SAVE_ROOT/GUIOdyssey_processed`.

</details>

<details>
<summary><b>ğŸ’³ AMEX</b></summary>
<br>

1. Download raw data from the [AMEX HF Repo](https://huggingface.co/datasets/Yuxiang007/AMEX).
2. Organize the data:
   ```
   root/
   â”œâ”€â”€ AMEX/
   â”‚   â”œâ”€â”€ element_anno/
   â”‚   â”œâ”€â”€ screenshot/
   â”‚   â””â”€â”€ metadata/
   ```
3. Modify `DATA_ROOT`, `SAVE_ROOT`, and `SPLIT` in `utils/data_utils/make_amex_data/make_amex_data.py`.
4. Run the script:
   ```bash
   python utils/data_utils/make_amex_data/make_amex_data.py
   ```
   Processed samples will be saved in `SAVE_ROOT/AMEX_processed`.

</details>

<details>
<summary><b>ğŸ­ GUIAct</b></summary>
<br>

1. Download raw data from the [GUIAct HF Repo](https://huggingface.co/datasets/yiye2023/GUIAct) by running `hf download yiye2023/GUIAct --repo-type dataset --local-dir path/to/GUICourse/GUIAct`.
2. Organize the data:
   ```
   root/
   â”œâ”€â”€ GUICourse/
   â”‚   â”œâ”€â”€ GUIAct/
   â”‚   â”‚   â”œâ”€â”€ smartphone_test_data.json
   â”‚   â”‚   â”œâ”€â”€ smartphone_test_images.parquet
   â”‚   â”‚   â”œâ”€â”€ smartphone_train_data.json
   â”‚   â”‚   â””â”€â”€ ...
   ```
3. Modify `DATA_ROOT`, `SAVE_DIR`, `CURRENT_SPLIT`, and `CURRENT_DEVICE_TYPE` in the `DatasetConfig` class within `utils/data_utils/make_guicourse_data/make_guicourse_data.py`.
4. Run the script:
   ```bash
   python utils/data_utils/make_guicourse_data/make_guicourse_data.py
   ```
   Processed samples will be saved in `SAVE_DIR`.

</details>

---

<div align="center">

## ğŸ”¬ **Technical Deep Dive**

*Advanced technical details for researchers and developers*

</div>

### ğŸ® **Unified Action Space Design**

<details>
<summary><b>ğŸ“± Mobile Action Framework</b></summary>
<br>

```json
{
  "mobile_actions": [
    "tap", "long_press", "drag", "input_text",
    "navigate_home", "navigate_back", "navigate_recent",
    "press_enter", "swipe", "wait", "status_complete"
  ]
}
```

</details>

<details>
<summary><b>âš¡ Unified Swipe Action</b></summary>
<br>

```json
{
  "action": "swipe",
  "start": [x, y],          // Starting coordinates
  "direction": "up",        // Movement direction  
  "distance": 200           // Swipe distance in pixels
}
```

</details>

---

<div align="center">

## ğŸ§ª **Model Usage**

</div>

**Stage 1: GUI Element Grounding**

Follow the code below to test the [stage 1 model for GUI element grounding](https://huggingface.co/HongxinLi/UIPro-7B_Stage1) evaluation.

```
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info

# Default: Load the model on the available device(s)
model = Qwen2VLForConditionalGeneration.from_pretrained(
    "HongxinLi/UIPro_1stage", torch_dtype="auto", device_map="auto"
)
processor = AutoProcessor.from_pretrained("HongxinLi/UIPro_1stage")

messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "image",
                "image": "./web_6f93090a-81f6-489e-bb35-1a2838b18c01.png",
            },

            # For ScreenSpot-v2, MOTIF, RefExp, and VisualWebBench Action Grounding
            {"type": "text", "text": "I want to {goal_info}. Please locate the target element I should interact with. (Output the center coordinates of the target)"},
            # For AutoGUI
            {"type": "text", "text": "Locate the element according to its detailed functionality description. {goal_info} (Output the center coordinates of the target)"},
            # For VisualWebBench Element Grounding
            {"type": "text", "text": "Locate the text "{goal_info}" (Output the center coordinates of the target)"},
        ],
    }
]

```


**Stage 2: Web Agent Embodiment**

Follow the code below to test the [stage 2 model for Web agent](https://huggingface.co/HongxinLi/UIPro-7B_Stage2_Web) task evaluation.

```
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info

# Default: Load the model on the available device(s)
model = Qwen2VLForConditionalGeneration.from_pretrained(
    "HongxinLi/UIPro-7B_Stage2_Web", torch_dtype="auto", device_map="auto"
)
processor = AutoProcessor.from_pretrained("HongxinLi/UIPro-7B_Stage2_Web")

messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "image",
                "image": "./web_6f93090a-81f6-489e-bb35-1a2838b18c01.png",
            },

            {"type": "text", "text": """Given the Web UI screenshot and previous actions, please generate the next move necessary to advance towards task completion. The user's task is: {task}
Action history: {action_history}

Now, first describe the action intent and then directly plan the next action."""},
        ],
    }
]

```


**Stage 2: Mobile Agent Embodiment**

Follow the code below to test the [stage 2 model for Mobile agent](https://huggingface.co/HongxinLi/UIPro-7B_Stage2_Mobile) task evaluation.

```
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info

# Default: Load the model on the available device(s)
model = Qwen2VLForConditionalGeneration.from_pretrained(
    "HongxinLi/UIPro-7B_Stage2_Mobile", torch_dtype="auto", device_map="auto"
)
processor = AutoProcessor.from_pretrained("HongxinLi/UIPro-7B_Stage2_Mobile")

messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "image",
                "image": "./web_6f93090a-81f6-489e-bb35-1a2838b18c01.png",
            },

            {"type": "text", "text": """Given the Mobile UI screenshot and previous actions, please generate the next move necessary to advance towards task completion. The user's task is: {task}
Action history: {action_history}

Now, first describe the action intent and then directly plan the next action."""},
        ],
    }
]

```

---

<div align="center">

## ğŸ§ª **Reproducing Agentic Task Evaluation**

*Run the agentic task evaluation scripts for AiTW / AndroidControl / GUIAct / Mind2Web.*

</div>

> **Note**
> - These scripts perform **online action prediction** step-by-step (i.e., they will run model inference), so a GPU environment is recommended.
> - Outputs are saved as JSON files under `utils/eval_utils/eval_results/`.

<details>
<summary><b>ğŸ¤– AiTW (Android in the Wild) â€” `utils/eval_utils/eval_aitw.py`</b></summary>
<br>

**What you need**
- **AiTW screenshots** (see the *Agentic SFT Data Processing â†’ Android in the Wild (AiTW)* section below). You only need the image folder for evaluation.
- The script loads the **test annotations** from HuggingFace: `HongxinLi/AITW_test`.

**Expected images layout**
```
/path/to/AITW/aitw_images/
â”œâ”€â”€ general/
â”œâ”€â”€ googleapps/
â”œâ”€â”€ install/
â”œâ”€â”€ single/
â””â”€â”€ webshopping/
```

**Run**

```bash
python utils/eval_utils/eval_aitw.py \
  --pretrained "HongxinLi/UIPro_2stage_Mobile" \
  --imgs_dir "/path/to/AITW/aitw_images" \
  --action_refexp \
  --scale 1000
```

**Common options**
- `--debug`: only evaluate a few steps for quick sanity check
- `--cot`: enable Chain-of-Thought prompting
- `--action_refexp`: used to drive the UIPro model to output a short action intent before predicting the action function call.
- `--max_prev_acts`: how many previous actions are included in history
- `--original_actspace`: use the original AITW action space in prompts. Ignore this to use the unified action space provided by UIPro.

**Outputs**
- Saved to `utils/eval_utils/eval_results/AITW/<model_postfix>[_CoT][_wActRef]/<timestamp>.json`

</details>

<details>
<summary><b>ğŸ® AndroidControl â€” `utils/eval_utils/eval_androidcontrol.py`</b></summary>
<br>

**What you need**
- A local AndroidControl test JSON (e.g., the processed file produced by `utils/data_utils/make_androidcontrol_data/make_androidcontrol_data.py`).
- The images should be reachable by joining `--data_root` with the `image` field in the test JSON.

We also provide a preprocessed zip file of the AndroidControl test data used in the UIPro paper. First download and unzip the GoClick AndroidControl Test Data via
`hf download HongxinLi/AndroidControl_test  --repo-type dataset --local-dir path/to/AndroidControl_test`

**Run**
```bash
python utils/eval_utils/eval_androidcontrol.py \
  --pretrained "HongxinLi/UIPro_2stage_Mobile" \
  --testset_path "utils/eval_utils/AndroidControl-test_12685.json" \
  --data_root "/path/to/AndroidControl"
```

**Useful options**
- `--action_refexp`: (only used for UIPro) Please use this to prompt UIPro to output short reasoning before planning the action 

**Outputs**
- Saved to `utils/eval_utils/eval_results/androidcontrol/<model_postfix>/<timestamp>.json`

</details>

<details>
<summary><b>ğŸ­ GUIAct â€” `utils/eval_utils/eval_guiact.py`</b></summary>
<br>

**What you need**
- A local GUIAct processed folder that contains the test JSON and images referenced by the `image` field.
- You can pass `--root` to point to your processed GUIAct directory, and `--imgs_dir` to override the image base.
  If omitted, the script auto-detects local/server paths inside `get_dataset_paths()` in `utils/eval_utils/eval_guiact.py`.

We also provide the preprocessed GUIAct test data zip file. First download the GUIAct data from HongxinLi/GUIAct, unzip it, and organize it as follows:
```
root/
â”œâ”€â”€ GUICourse/
â”‚   â””â”€â”€ GUIAct/
â”‚   â”‚    â””â”€â”€ imgs
â”‚   â”œâ”€â”€ Web_test.json
â”‚   â””â”€â”€ Mobile_test.json
```


**Run**
```bash
python utils/eval_utils/eval_guiact.py \
  --pretrained "HongxinLi/UIPro-7B_Stage2_Web" \
  --device_type Web \
  --scale 1000 \
  --root "/path/to/GUICourse_processed" \
  --imgs_dir "/path/to/images_base"
```

**Common options**
- `--device_type`: `Web` or `Mobile`
- `--root`: processed GUIAct directory (overrides auto-detect)
- `--imgs_dir`: image base directory (overrides auto-detect)
- `--debug`: evaluate a small random subset
- `--cot`: enable Chain-of-Thought prompting
- `--action_refexp`: (only used for UIPro) Please use this to prompt UIPro to output short reasoning before planning the action 
- `--original_actspace`: use the original GUIAct action space

**Outputs**
- Saved to `utils/eval_utils/eval_results/GUIAct-<device_type>/<model_postfix>/<timestamp>.json`

</details>

<details>
<summary><b>ğŸŒ Mind2Web â€” `utils/eval_utils/eval_mind2web.py`</b></summary>
<br>

**What you need**
- A local Mind2Web folder that contains:
  - `mind2web_images/` (evaluation screenshots)
  - `mind2web_data_test_{website|task|domain}.json` (test annotations)
- You can pass `--root` to point to your local Mind2Web directory. If omitted, the script auto-selects
  the first existing path in `DEFAULT_ROOTS` inside `utils/eval_utils/eval_mind2web.py`.

**Expected layout**
```
/path/to/Mind2Web/
â”œâ”€â”€ mind2web_images/
â””â”€â”€ mind2web_data_test_website.json
    mind2web_data_test_task.json
    mind2web_data_test_domain.json
```

**Run**
```bash
python utils/eval_utils/eval_mind2web.py \
  --pretrained "HongxinLi/UIPro-7B_Stage2_Web" \
  --task website \
  --scale 1000 \
  --root "/path/to/Mind2Web"
```

**Common options**
- `--task`: either 'task', 'domain', or 'website' (the three splits of Mind2Web)
- `--debug`: only evaluate a few episodes
- `--cot`: enable Chain-of-Thought prompting
- `--action_refexp`: (only used for UIPro) Please use this to prompt UIPro to output short reasoning before planning the action 
- `--max_prev_acts`: how many previous actions are included in history

**Outputs**
- Saved to `utils/eval_utils/eval_results/mind2web/<model_postfix>/{website|task|domain}-<timestamp>.json`

</details>

<div align="center">

---

<div align="center">

## ğŸ“ **Citation**

*If you use UIPro in your research, please cite our paper*

</div>

```bibtex
@inproceedings{li2025uipro,
  title={UIPro: Unleashing Superior Interaction Capability For GUI Agents},
  author={Li, Hongxin and Su, Jingran and Chen, Jingfan and Ju, Zheng and Chen, Yuntao and Li, Qing and Zhang, Zhaoxiang},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  year={2025}
}
```

---

<div align="center">

## ğŸ‘¥ **Team & Acknowledgments**

*Special thanks to our research team and the open-source community*

<p>
This work was supported in part by the <b>National Key R&D Program of China</b> and the <b>National Natural Science Foundation of China</b>. We extend our gratitude to the open-source community for providing foundational datasets and tools that made this research possible.
</p>

<br>

---

<br>

## â­ **Star this repository if you find UIPro helpful!** â­

<a href="https://github.com/ZJULiHongxin/UIPro/stargazers">
  <img src="https://img.shields.io/github/stars/ZJULiHongxin/UIPro?style=for-the-badge&logo=github&logoColor=white&labelColor=1A1A2E&color=FFD93D" alt="GitHub Stars"/>
</a>

<br><br>

<p><i>ğŸš€ Revolutionizing GUI automation, one interaction at a time</i></p>

</div>
